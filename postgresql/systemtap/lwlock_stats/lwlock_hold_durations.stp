#
# This tapscript requires a postgres patched with my lwlock tracepoint enhancements.
#
# It requires the absolute path to the target postgres executable as the first
# positional argument after the script.
#
# You'll have to run with resource limits turned off for this to work since the
# script is very expensive and not optimised right now. Use 'stap -t' if you
# want to see probe workload and time spent. It spends quite a lot of time on
# lock contention for various global arrays right now.
#
# I use
#
#	 stap -v -g --suppress-time-limits
#
# A typical invocation with some parameters might be:
#
#	 /usr/local/systemtap/bin/stap -v -g --suppress-time-limits -G track_per_process=1 -G cumulative=1 -G log_waits_longer_than_us=9000 -G log_holds_longer_than_us=20000 -G track_blockers=1 -G hide_wait_total_summary_lt_us=10000 -G hide_held_total_summary_lt_us=20000 -G hide_wait_total_pid_lt_us=1000 -G hide_held_total_pid_lt_us=5000 lwlock_hold_durations.stp /path/to/your/postgres
#
# This script assumes only one postmaster is running with the target
# executable. It'd be pretty simple to add filters for postmaster pid to target
# one postmater if you wanted though. Or even to group all the arrays under
# per-postmaster sets.
#
# PARAMETERS
#
#	 -G hide_wait_total_summary_lt_us=0
#	 -G hide_held_total_summary_lt_us=0
#	 -G hide_wait_total_pid_lt_us=0
#	 -G hide_held_total_pid_lt_us=0
#
#		 Suppress display of wait or hold entries whose totals are less than
#		 the supplied values. Helps reduce noise in output.
#
#	 -G track_per_process=0
#
#		 Produce summary output but omit per-process-id tracking of locks.
#		 Signficantly reduces overhead and reduces the volume of output.
#
#	 -G periodic_summary=0
#
#		 Suppress the periodic updates of the summary counter.
#
#	 -G cumulative=0
#
#		 Reset allprocs samples each time interval. Does not affect
#		 per-process samples since they're only emitted once at process exit
#		 anyway. Has no effect if periodic_summary=0.
#
#	 -G log_waits_longer_than_us=50000
#	 -G log_holds_longer_than_us=50000
#
#		 Report individual waits or holds if they are long. Time in
#		 microseconds above which the wait or hold is reported.
#
#		 If debuginfo is available this will report the application_name or,
#		 if unset, the backend type, for both waiter and holder if possible.
#
#	 -G track_application_names=0
#
#		 Don't attempt to track application names and backend IDs. Only
#		 affects log_waits_longer_than_us and log_holds_longer_than_us.
#
#		 Turning this off saves some runtime.
#
#	 -G track_holds=0
#
#	     Don't keep track of lock holds. Only lock waits will be collected
#	     and reported.
#
#	     If track_blockers=1 then holds will still be tracked even when this
#	     is off, but stats accumulation will be suppressed.
#
#	 -G track_blockers=0
#
#		 Don't try to track blocker for waits. Only affects
#		 log_waits_longer_than_us and log_holds_longer_than_us.
#
#		 Only the first blocker is recorded. If a lock acquire is woken and
#		 goes back to sleep waiting for a different blocker, all wait time
#		 will be attributed to the first blocker.
#
#		 Turning this off saves some runtime.
#
#
# All these parameters can be tweaked during script execution if using
# stap --monitor mode.
#
# TODO measure time lock is held while contested separately
# TODO filter out timings for locks that were uncontested and short
# TODO support tracking by MyBackendType category
# TODO hard code built-in tranche names as optimisation?
# TODO trace point for tranche init that can be used to capture tranche names
#
#
# Performance:
#
# * 0:16 untracked
# * 2:27 track long waits, track app names, track blockers, -track pids
# * 2:30 with most stuff off too!

%( $# > 0 %?
	@define POSTGRES_PATH %( @1 %)
%:
	@define POSTGRES_PATH %( "/home/craig/projects/2Q/postgres/dev/lwlock-tracepoints/build/tmp_install/home/craig/pg/lwlock-tracepoints/bin/postgres" %)
%)

// These globals control options that can be set with -G flags on the
// command line. See the header comment.
global track_holds = 1
global periodic_summary = 1
global track_per_process = 1
global cumulative = 1
global log_waits_longer_than_us = 0
global log_holds_longer_than_us = 0
global backtrace_long_waits_or_holds = 1
global track_blockers = 0
global track_application_names = 1
global hide_wait_total_summary_lt_us = 0
global hide_held_total_summary_lt_us = 0
global hide_wait_total_pid_lt_us = 0
global hide_held_total_pid_lt_us = 0

// Debug mode that turns off stats accumulation
@define TRACK_STATS %( 1 %)

// Sanity checking for script development and debugging.
//
// This assumes you start the trace before the postmaster so that it can see
// the whole process lifetime.
//
@define SANITY_CHECKS %( 1 %)

probe postgres = process(@POSTGRES_PATH) {}

// Track currently-held locks by pid. Indexed by [lwlock_p, pid(), mode] => timestamp acquired
private locks_held_by_pid;

// For lock waits in progress track the waiting pid's start wait time
private lock_wait_start_us;
// and the lock being acquired
private lock_wait_lwlock_p;
// For lwlock wait tracking of the blocking pid, if it can be found. Also
// blocking pid's application_name at the time we noticed it was blocking.
private lock_wait_blocker;
private lock_wait_blocker_mode;
private lock_wait_blocker_appname;

// The acquire function we're in right now. By pid.
private acquire_func;

// If we're tracking application names, track the application_name of each pid
private application_names;
// Track MyBackendType for each backend too, if we're tracking appnames.
private backend_types;

// Stats arrays indexed by [tranche_id, mode], for lock wait stats and lock hold stats
private held_durations_allprocs[1000];
private wait_durations_allprocs[1000];

// Stats array indexed by [pid(), tranche_id, mode]
//
// This array must be very large if we're going to accumulate all pid stats so
// instead we write stats for each pid once it exits. Also let this array cycle
// (% suffix)
private held_durations_pid[5000];
private wait_durations_pid[5000];

// tranche id -> name mapping, since we don't want to rely on seeing
// the tranches registered at startup.
private tranche_names;

// enum LWLockMode
@define LW_EXCLUSIVE		%( 0 %)
@define LW_SHARED		   %( 1 %)
@define LW_WAIT_UNTIL_FREE  %( 2 %)

function remember_tranche_name(tranche_id:long, tranche_name_p:long) {
	if (!([tranche_id] in tranche_names)) {
		// This assumes tranche id->name is consistent across procs. That's not
		// guaranteed, but it's close enough for this purpose.
        //
        // Should really optimise this better.
        //
		tranche_names[tranche_id] = user_string(tranche_name_p);
	}
}

# Hold tracking. We're not interested in paths that fail to take a lock here,
# only actual acquires, and there's a single tracepoint for that.
probe postgres.mark("lwlock__acquired") if (track_holds || track_blockers) {
    tranche_name_p = $arg1
	mode = $arg2
	lwlock_p = $arg3
	tranche_id = $arg4
	assert (mode == @LW_EXCLUSIVE || mode == @LW_SHARED, sprintf("invalid lockmode %d", mode));
    remember_tranche_name(tranche_id, tranche_name_p)
    locks_held_by_pid[pid(), lwlock_p, mode] = gettimeofday_us();
}

# Lock release for hold tracking. Wait tracking uses different probes.
probe postgres.mark("lwlock__release") if (track_holds || track_blockers) {
	//tranche_name = user_string($arg1)
	mode = $arg2
	lwlock_p = $arg3
	tranche_id = $arg4
	assert (mode == @LW_EXCLUSIVE || mode == @LW_SHARED, sprintf("invalid lockmode %d", mode));
	acquired_us = locks_held_by_pid[pid(), lwlock_p, mode]
	// If no acquire time is recorded we probably started tracing after
	// the lock was acquired, so ignore.
	if (acquired_us != 0) {
		released_us = gettimeofday_us()
		held = released_us - acquired_us
        if (track_holds)
        {
            append_held_stats(held, mode, tranche_id)
            report_long_holds(held, mode, tranche_id)
        }
	}
	delete locks_held_by_pid[pid(), lwlock_p, *]
}

// LWLock waits. Note that we may wait multiple times for a given acquire. We don't presently
// try to keep track of the total wait time for each acquire, we treat each wait as individual
// incidents.
//
// We want to accumulate each wait into a total since the number of wakeups is less interesting
// than the total time waited. We won't bother counting the wakeups.
//
probe postgres.mark("lwlock__wait__start") {
	mode = $arg2;
	lwlock_p = $arg3;
	tranche_id = $arg4;
	// Only record the wait-start time if this is the first wait for the
	// lock acquire. We don't want to forget the first wait time if a prior
	// wakeup failed to get the lock.
	//
	// NOTE: This only gets captured the first time we start waiting, so if
	// the holder changes we don't update the info.
	//
	if (!([pid()] in lock_wait_start_us))
	{
		lock_wait_start_us[pid()] = gettimeofday_us()
        if (@SANITY_CHECKS) {
		    lock_wait_lwlock_p[pid()] = lwlock_p
        }

		if (track_blockers) {
            track_blocker_info_for_wait(lwlock_p, mode)
		}
	}
	else
	{
        if (@SANITY_CHECKS) {
            assert(lock_wait_lwlock_p[pid()] == lwlock_p,
                        sprintf("wait resume lwlock ptr mismatch: got %p expected %p", lock_wait_lwlock_p[pid()], lwlock_p));
        }
	}

}

function track_blocker_info_for_wait(lwlock_p:long, mode:long)
{
    // This is how to get the pid that holds the lock now. It's slow though. And
    // we could land up waiting on a different holder each time we wake if it's
    // quite contested. So this is optional.
    //
    // It's unclear why trying to loop over "[*, lwlock_p,
    // *]" fails with "unresolved symbol: lwlock_p" but as
    // a workaround, scan the array manually. It probably
    // does much the same thing anyway.
    //
    // TODO: maintain a reverse-map of lock-ptr =>
    // lock-holder for faster scans? Vs cost of maintenance...
    //
    if (mode == @LW_SHARED)
    {
        foreach ([blocker_pid, entry_lwlock_p, holder_mode] in locks_held_by_pid[*, *, @LW_EXCLUSIVE])
        {
        if (entry_lwlock_p == lwlock_p)
            break;
        }
    }
    else
    {
        foreach ([blocker_pid, entry_lwlock_p, holder_mode] in locks_held_by_pid[*, *, *])
        {
        if (entry_lwlock_p == lwlock_p)
            break;
        }
    }

    lock_wait_blocker[pid()] = blocker_pid;
    lock_wait_blocker_mode[pid()] = holder_mode;

    // Save blocker's application name or backend
    // type now, since it might be cleared due to
    // blocker exit before we notice the lock
    // release and go to report it.
    lock_wait_blocker_appname[pid()] = application_names[blocker_pid]
}

/*
 * We don't use the lwlock__wait__done event here. Because a single lock acquire
 * may do many waits, we're more interested in the total time between the first
 * wait and the actual acquire.
 */
/*
probe postgres.mark("lwlock__wait__done") {
	mode = $arg2;
	lwlock_p = $arg3;
	tranche_id = $arg4;
	wait_start_us = lock_wait_start_us[pid(), lwlock_p];
}
*/

/*
 * So track the individual lock acquire events specifically.
 *
 * TODO provide a single event for all "done waiting" cases.
 */
probe lwlock_acquired = postgres.mark("lwlock__acquired")
{
	acquired = 1
	mode = $arg2
	lwlock_p = $arg3
	tranche_id = $arg4
}
// LWLockWaitForVar doesn't acquire the lock so it doesn't hit
// lwlock__acquired. Handle separately.
probe lwlock_waitforvar_done = postgres.mark("lwlock__waitforvar__done")
{
	acquired = 0
	mode = @LW_EXCLUSIVE
	lwlock_p = $arg2
	tranche_id = $arg3
	// oldval = $arg5
	// newval = $arg6
	// result = $arg7
}
// Same for the fail-path for condacquire or AcquireOrWait. The success-paths
// hits lwlock__acquired.
probe lwlock_condacquire_fail = postgres.mark("lwlock__condacquire__fail"),
				postgres.mark("lwlock__acquire__or__wait__fail")
{
	acquired = 0
	mode = $arg2
	lwlock_p = $arg3
	tranche_id = $arg4
}

probe lwlock_acquired, lwlock_waitforvar_done, lwlock_condacquire_fail
{
	wait_start_us = lock_wait_start_us[pid()];

	// Ignore uncontested locks where we didn't wait, or locks where
	// we didn't see the start-event.
	//
	// TODO count uncontested acquires too.
	//
	if (wait_start_us != 0) {
        if (@SANITY_CHECKS) {
            assert(lock_wait_lwlock_p[pid()] == lwlock_p,
                    sprintf("mismatched lwlock pointer in acquired: got %p expected %p (in %s)", lock_wait_lwlock_p[pid()], lwlock_p, acquire_func[pid()]))
        }
		wait_end_us = gettimeofday_us();
		waited = wait_end_us - wait_start_us;

        append_wait_stats(waited, mode, tranche_id)
        report_long_waits(waited, mode, tranche_id)

	}
    // Clear wait track state. We might be tracking holds, so don't just
    // delete_per_pid_entries() here.
	delete lock_wait_start_us[pid()]
    if (track_blockers) {
        delete lock_wait_blocker[pid()]
        delete lock_wait_blocker_mode[pid()]
        delete lock_wait_blocker_appname[pid()]
    }
	delete acquire_func[pid()]
    if (@SANITY_CHECKS) {
	    delete lock_wait_lwlock_p[pid()]
    }
}

/*
 * SANITY CHECK
 *
 * If we try to begin an acquire while another is ongoing, that must be a trace
 * script bug. Detect it.
 */
probe postgres.mark("lwlock__acquire__start") if (@SANITY_CHECKS),
	  postgres.mark("lwlock__condacquire__start") if (@SANITY_CHECKS),
	  postgres.mark("lwlock__waitforvar__start") if (@SANITY_CHECKS)
{
	assert(!([pid()] in acquire_func),
			sprintf("already in acquire func %s when entering %s",
				acquire_func[pid()], $$name))
	assert(!([pid()] in lock_wait_start_us),
			sprintf("already waiting when entering %s", $$name))
	assert(!([pid()] in lock_wait_lwlock_p),
			sprintf("already have lwlock ptr %p when entering %s", lock_wait_lwlock_p[pid()], $$name))
    if (track_blockers) {
        assert(!([pid()] in lock_wait_blocker),
                sprintf("already have blocker when entering %s", $$name))
        assert(!([pid()] in lock_wait_blocker_appname),
                sprintf("already have blocker appname when entering %s", $$name))
    }
	acquire_func[pid()] = $$name;
}

/*
 * SANITY CHECK
 *
 * Must have reported lock acquire and cleared state when we finish acquire or
 * condacquire, so acquire_func must be clear.
 */
probe postgres.mark("lwlock__condacquire") if (@SANITY_CHECKS),
      postgres.mark("lwlock__acquire") if (@SANITY_CHECKS),
      postgres.mark("lwlock__acquire__or__wait") if (@SANITY_CHECKS)
{
	assert(!([pid()] in acquire_func),
		sprintf("expected empty acquire_func in %s but got %s", $$name, acquire_func[pid()]))
}

/*
 * Identify backend type on startup and set a default for the description shown. Uses
 * the backend__type probe, included in the patchset.
 *
 * If application_name is set, we'll override this with the app name later.
 */
probe postgres.mark("backend__type") if (track_application_names) {
	backend_types[pid()] = $arg1
    if ($arg2 != 0)
	    application_names[pid()] = user_string($arg2, "")
}
probe postgres.mark("guc__application__name__assigned") if (track_application_names) {
	if ($arg1 != 0)
		application_names[pid()] = user_string($arg1,"")
}

/*
 * We have to clean up entries for all the per-pid arrays manually
 * to ensure we don't leak and fill the arrays.
 *
 * Systemtap really needs "private threadlocal" variables...
 */
function delete_per_pid_entries()
{
    if (track_holds || track_blockers)
    {
        foreach (acquired_us = [pid, lwlock_p, held_mode] in locks_held_by_pid[pid(), *, *])
        {
            leaked_locks ++;
            printf("[%6d] leaked LWLock %p (mode %d)!\n", pid, lwlock_p, held_mode);
        }
        delete locks_held_by_pid[pid(), *, *]
    }

    if (track_blockers) {
        delete lock_wait_blocker[pid()]
        delete lock_wait_blocker_mode[pid()]
        delete lock_wait_blocker_appname[pid()]
    }

    if (track_application_names) {
        delete application_names[pid()]
        delete backend_types[pid()]
    }

	delete lock_wait_start_us[pid()]

	delete acquire_func[pid()]

    if (@SANITY_CHECKS) {
	    delete lock_wait_lwlock_p[pid()]
    }
}

function tranche_id_str:string(tranche_id:long) {
	n = tranche_names[tranche_id]
	return n != "" ? n : sprintf("<tranche_id %d>", tranche_id)
}

function lockmode_str:string(mode:long) {
	if (mode == @LW_EXCLUSIVE) {
		return "E"
	} else if (mode == @LW_SHARED) {
		return "S"
	} else if (mode == @LW_WAIT_UNTIL_FREE) {
		return "W"
	} else {
		error(sprintf("unknown lockmode %d", mode));
	}
}

function report_long_holds(held_us:long, mode:long, tranche_id:long) {
    if (log_holds_longer_than_us > 0 && held_us > log_holds_longer_than_us) {
        appname = application_names[pid()]
        backend_type = backend_types[pid()]
        if (appname != "") {
            appname = " (" . appname . ")"
        }
        printf("!!H!! [%6d]:%-3d %20s %1s %8d (%10s)%s\n",
                pid(), backend_type, tranche_id_str(tranche_id),
                lockmode_str(mode),
                held_us, usecs_to_string(held_us),
                appname);
    }
}

function report_long_waits(waited_us:long, mode:long, tranche_id:long) {
    if (log_waits_longer_than_us > 0 && waited_us > log_waits_longer_than_us) {
		func = acquire_func[pid()]
        backend_type = backend_types[pid()]
        msg = sprintf("!!W!! [%6d]:%-3d %20s %1s %8d (%10s) in %s",
                pid(), backend_type, tranche_id_str(tranche_id),
                lockmode_str(mode),
                waited_us, usecs_to_string(waited_us), func);
        appname = application_names[pid()]
        if (appname != "") {
            msg .= " (" . appname . ")"
        }
        if (track_blockers) {
            blocked_by = lock_wait_blocker[pid()];
            if (blocked_by != 0) {
                blocker_mode = lock_wait_blocker_mode[pid()];
                msg .= sprintf(" => [%6d] %s", blocked_by, lockmode_str(blocker_mode))
            }
            blocker_appname = lock_wait_blocker_appname[pid()]
            if (blocker_appname != "") {
                msg .= " (" . blocker_appname . ")"
            }
        }
        println(msg);
    }
}

@define append_stats(statvar_allprocs, statvar_pid, duration_us, mode, tranche_id) %(
if (@TRACK_STATS)
{
    // Summary across all pids by lockmode, and by (@tranche_id, lockmode)
    @statvar_allprocs[-1, @mode] <<< @duration_us;
    @statvar_allprocs[@tranche_id, @mode] <<< @duration_us;

    // TODO: By backend-type
    //statvar_backend_type[betype[pid()], -1, @mode] <<< @duration_us;
    //statvar_backend_type[betype[pid()], @tranche_id, @mode] <<< @duration_us;

    // And for this process
    if (track_per_process) {
        @statvar_pid[pid(), @tranche_id, @mode] <<< @duration_us;
        @statvar_pid[pid(), -1, @mode] <<< @duration_us;
    }
}
%)

function append_held_stats(held_us:long, mode:long, tranche_id:long) {
    @append_stats(held_durations_allprocs, held_durations_pid, held_us, mode, tranche_id)
}

function append_wait_stats(waited_us:long, mode:long, tranche_id:long) {
    @append_stats(wait_durations_allprocs, wait_durations_pid, waited_us, mode, tranche_id)
}

function print_stats_header(label) {
	printf("%-20s %30s %4s %8s %12s %8s %8s %8s %8s\n", label, "tranche", "mode", "count", "total", "avg", "variance", "min", "max");
}

/*
 * Stat printing in systemtap is verbose, tidy it a bit. We can't pass stats
 * values as arguments, so we have to use a macro to copy the stats-expression
 * to the various functions used.
 */
@define PRINT_STATS_ENTRY(statexpr, label, tranche_id, mode, hide_total_threshold_us) %(
{
	stat_count = @count(@statexpr)
	stat_total = @sum(@statexpr)
	if (stat_count > 0
		&& (@hide_total_threshold_us == 0 || stat_total > @hide_total_threshold_us))
    {
	    tranche_name = @tranche_id == -1 ? "(all)" : tranche_id_str(@tranche_id)
		printf("%-20s %30s %4s %8d %12d %8d %8d %8d %8d\n",
				@label, tranche_name, lockmode_str(@mode),
				stat_count, stat_total,
				@avg(@statexpr),
				@variance(@statexpr),
				@min(@statexpr),
				@max(@statexpr))
    }
}
%)

function print_held_stats_summary(label:string,fortranche:long,formode:long)
{
    @PRINT_STATS_ENTRY(held_durations_allprocs[fortranche,formode], label, fortranche, formode, hide_held_total_summary_lt_us)
}

function print_wait_stats_summary(label:string,fortranche:long,formode:long)
{
    @PRINT_STATS_ENTRY(wait_durations_allprocs[fortranche,formode], label, fortranche, formode, hide_wait_total_summary_lt_us)
}

function print_stats_pid(forpid:long,fortranche:long,formode:long,atexit:long)
{
    label = sprintf("%1s  h [%06d]", atexit == 0 ? "*" : " ", forpid)
    @PRINT_STATS_ENTRY(held_durations_pid[forpid,fortranche,formode], label, fortranche, formode, hide_held_total_pid_lt_us)
    @PRINT_STATS_ENTRY(wait_durations_pid[forpid,fortranche,formode], label, fortranche, formode, hide_wait_total_pid_lt_us)
}

function print_stats() {
	// Suppress action if nothing collected at all
	if ([*, *] in held_durations_allprocs) {

		// By-mode rollup
		printf("\n")
		print_stats_header("held locks: all procs")
		print_held_stats_summary("  H LW_EXCLUSIVE", -1, @LW_EXCLUSIVE);
		print_held_stats_summary("  H LW_SHARED", -1, @LW_SHARED);

		// By tranche id and mode, all procs
		printf("\n")
		print_stats_header("all procs by tranche")
		foreach ([tranche_id+, mode] in held_durations_allprocs[*, *]) {
			print_held_stats_summary("  H tranche ", tranche_id, mode)
		}
	}

	if ([*, *] in wait_durations_allprocs) {

		// By-mode rollup
		printf("\n")
		print_stats_header("wait locks: all procs")
		print_wait_stats_summary("  W LW_EXCLUSIVE", -1, @LW_EXCLUSIVE);
		print_wait_stats_summary("  W LW_SHARED", -1, @LW_SHARED);

		// By tranche id and mode, all procs
		printf("\n")
		print_stats_header("all procs by tranche")
		foreach ([tranche_id+, mode] in wait_durations_allprocs[*, *]) {
			print_wait_stats_summary("  W tranche ", tranche_id, mode)
		}
		printf("------\n");
	}

	if (! cumulative) {
		delete held_durations_allprocs[*, *];
		delete wait_durations_allprocs[*, *];
	}

}

probe timer.ms(5000) if (periodic_summary) {
	print_stats();
}

// We print stats on a proc when it exits so we can forget it and not bloat the
// arrays too much.
probe postgres.end if (track_per_process) {
	if ([pid(), *, *] in held_durations_pid) {
		backend_type = backend_types[pid()]
		backend_description = application_names[pid()]
		printf("[%6d] backend_type=%-3d (%s) exited\n", pid(), backend_type, backend_description)
		print_stats_header(sprintf("[%6d]:", pid()))

		foreach ([pid, tranche_id+, mode] in held_durations_pid[pid(), *, *]) {
			print_stats_pid(pid, tranche_id, mode, 1)
		}

		delete held_durations_pid[pid(), *, *];

		printf("=======\n");
	}
}

// Prevent resource leaks. This must appear after the other .end probes since
// they might refer to the globals we're clearing.
probe postgres.end {
	delete_per_pid_entries()
}

probe end {
	print_stats();

	if (track_per_process)
	{
		if ([*, *, *] in held_durations_pid) {
			print_stats_header("all remaining backends by pid")
			foreach ([pid, tranche_id, mode] in held_durations_pid[pid, *, *]) {
				print_stats_pid(pid, tranche_id, mode, 0)
			}
		}
	}

	printf("------\n");
}

# vim: ts=4 et sw=4 ai
